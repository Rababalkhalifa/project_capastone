{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Build a language detector model\n",
    "The goal of this exercise is to train a linear classifier on text features\n",
    "that represent sequences of up to 3 consecutive characters so as to be\n",
    "recognize natural languages by using the frequencies of short character\n",
    "sequences as 'fingerprints'.\n",
    "\"\"\"\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: Simplified BSD\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data\\Tweets_x.csv', delimiter='\\t',names= [\"Tweet\", \"Classification\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset['Tweet'], dataset['Classification'], test_size=0.5)\n",
    "\n",
    "\n",
    "# TASK: Build a vectorizer that splits strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(dataset['Tweet'])\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "#return how many word has occured in a document\n",
    "print(count_vect.vocabulary_.get(u'الحرية'))\n",
    "\n",
    "# TASK: Build a vectorizer / classifier pipeline using the previous analyzer\n",
    "# the pipeline instance should stored in a variable named clf\n",
    "\n",
    "# TASK: Fit the pipeline on the training set\n",
    "\n",
    "# TASK: Predict the outcome on the testing set in a variable named y_predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** word occured in a document with counting **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(count_vect.vocabulary_)\n",
    "\n",
    "words_tokenized = pd.DataFrame(count_vect.vocabulary_.values(),columns=['frequency'],index=count_vect.vocabulary_.keys())\n",
    "words_tokenized.sort_values(ascending=False,by='frequency').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "https://www.geeksforgeeks.org/python-stemming-words-with-nltk/\n",
    "https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Home/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Home\\\\Anaconda2\\\\nltk_data'\n    - 'C:\\\\Users\\\\Home\\\\Anaconda2\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Home\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a9dd8040f6bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marb_stopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arabic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Home\\Anaconda2\\lib\\site-packages\\nltk\\corpus\\util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Home\\Anaconda2\\lib\\site-packages\\nltk\\corpus\\util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Home/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Home\\\\Anaconda2\\\\nltk_data'\n    - 'C:\\\\Users\\\\Home\\\\Anaconda2\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Home\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "arb_stopwords = set(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = count_vect.vocabulary_.keys()\n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.matshow(cm, cmap=plt.cm.jet)\n",
    "#plt.show()\n",
    "\n",
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    u'This is a language detection test.',\n",
    "    u'Ceci est un test de d\\xe9tection de la langue.',\n",
    "    u'Dies ist ein Test, um die Sprache zu erkennen.',\n",
    "]\n",
    "predicted = clf.predict(sentences)\n",
    "\n",
    "for s, p in zip(sentences, predicted):\n",
    "    print(u'The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authors: Eustache Diemert <eustache@diemert.fr>\n",
    "#          @FedericoV <https://github.com/FedericoV/>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from glob import glob\n",
    "import itertools\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.externals.six.moves import html_parser\n",
    "from sklearn.externals.six.moves.urllib.request import urlretrieve\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def _not_in_sphinx():\n",
    "    # Hack to detect whether we are running by the sphinx builder\n",
    "    return '__file__' in globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n",
    "                               alternate_sign=False)\n",
    "\n",
    "\n",
    "# We learn a binary classification between the \"acq\" class and all the others.\n",
    "# \"acq\" was chosen as it is more or less evenly distributed in the Reuters\n",
    "# files. For other datasets, one should take care of creating a test set with\n",
    "# a realistic portion of positive instances.\n",
    "all_classes = np.array([0, 1])\n",
    "positive_class = 'acq'\n",
    "\n",
    "# Here are some classifiers that support the `partial_fit` method\n",
    "partial_fit_classifiers = {\n",
    "    'SGD': SGDClassifier(max_iter=5),\n",
    "    'Perceptron': Perceptron(tol=1e-3),\n",
    "    'NB Multinomial': MultinomialNB(alpha=0.01),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(tol=1e-3),\n",
    "}\n",
    "\n",
    "\n",
    "def get_minibatch(doc_iter, size, pos_class=positive_class):\n",
    "    \"\"\"Extract a minibatch of examples, return a tuple X_text, y.\n",
    "\n",
    "    Note: size is before excluding invalid docs with no topics assigned.\n",
    "\n",
    "    \"\"\"\n",
    "    data = [(u'{title}\\n\\n{body}'.format(**doc), pos_class in doc['topics'])\n",
    "            for doc in itertools.islice(doc_iter, size)\n",
    "            if doc['topics']]\n",
    "    if not len(data):\n",
    "        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n",
    "    X_text, y = zip(*data)\n",
    "    return X_text, np.asarray(y, dtype=int)\n",
    "\n",
    "\n",
    "def iter_minibatches(doc_iter, minibatch_size):\n",
    "    \"\"\"Generator of minibatches.\"\"\"\n",
    "    X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "    while len(X_text):\n",
    "        yield X_text, y\n",
    "        X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "\n",
    "\n",
    "# test data statistics\n",
    "test_stats = {'n_test': 0, 'n_test_pos': 0}\n",
    "\n",
    "# First we hold out a number of examples to estimate accuracy\n",
    "n_test_documents = 1000\n",
    "tick = time.time()\n",
    "X_test_text, y_test = get_minibatch(data_stream, 1000)\n",
    "parsing_time = time.time() - tick\n",
    "tick = time.time()\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "vectorizing_time = time.time() - tick\n",
    "test_stats['n_test'] += len(y_test)\n",
    "test_stats['n_test_pos'] += sum(y_test)\n",
    "print(\"Test set is %d documents (%d positive)\" % (len(y_test), sum(y_test)))\n",
    "\n",
    "\n",
    "def progress(cls_name, stats):\n",
    "    \"\"\"Report progress information, return a string.\"\"\"\n",
    "    duration = time.time() - stats['t0']\n",
    "    s = \"%20s classifier : \\t\" % cls_name\n",
    "    s += \"%(n_train)6d train docs (%(n_train_pos)6d positive) \" % stats\n",
    "    s += \"%(n_test)6d test docs (%(n_test_pos)6d positive) \" % test_stats\n",
    "    s += \"accuracy: %(accuracy).3f \" % stats\n",
    "    s += \"in %.2fs (%5d docs/s)\" % (duration, stats['n_train'] / duration)\n",
    "    return s\n",
    "\n",
    "\n",
    "cls_stats = {}\n",
    "\n",
    "for cls_name in partial_fit_classifiers:\n",
    "    stats = {'n_train': 0, 'n_train_pos': 0,\n",
    "             'accuracy': 0.0, 'accuracy_history': [(0, 0)], 't0': time.time(),\n",
    "             'runtime_history': [(0, 0)], 'total_fit_time': 0.0}\n",
    "    cls_stats[cls_name] = stats\n",
    "\n",
    "get_minibatch(data_stream, n_test_documents)\n",
    "# Discard test set\n",
    "\n",
    "# We will feed the classifier with mini-batches of 1000 documents; this means\n",
    "# we have at most 1000 docs in memory at any time.  The smaller the document\n",
    "# batch, the bigger the relative overhead of the partial fit methods.\n",
    "minibatch_size = 1000\n",
    "\n",
    "# Create the data_stream that parses Reuters SGML files and iterates on\n",
    "# documents as a stream.\n",
    "minibatch_iterators = iter_minibatches(data_stream, minibatch_size)\n",
    "total_vect_time = 0.0\n",
    "\n",
    "# Main loop : iterate on mini-batches of examples\n",
    "for i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n",
    "\n",
    "    tick = time.time()\n",
    "    X_train = vectorizer.transform(X_train_text)\n",
    "    total_vect_time += time.time() - tick\n",
    "\n",
    "    for cls_name, cls in partial_fit_classifiers.items():\n",
    "        tick = time.time()\n",
    "        # update estimator with examples in the current mini-batch\n",
    "        cls.partial_fit(X_train, y_train, classes=all_classes)\n",
    "\n",
    "        # accumulate test accuracy stats\n",
    "        cls_stats[cls_name]['total_fit_time'] += time.time() - tick\n",
    "        cls_stats[cls_name]['n_train'] += X_train.shape[0]\n",
    "        cls_stats[cls_name]['n_train_pos'] += sum(y_train)\n",
    "        tick = time.time()\n",
    "        cls_stats[cls_name]['accuracy'] = cls.score(X_test, y_test)\n",
    "        cls_stats[cls_name]['prediction_time'] = time.time() - tick\n",
    "        acc_history = (cls_stats[cls_name]['accuracy'],\n",
    "                       cls_stats[cls_name]['n_train'])\n",
    "        cls_stats[cls_name]['accuracy_history'].append(acc_history)\n",
    "        run_history = (cls_stats[cls_name]['accuracy'],\n",
    "                       total_vect_time + cls_stats[cls_name]['total_fit_time'])\n",
    "        cls_stats[cls_name]['runtime_history'].append(run_history)\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            print(progress(cls_name, cls_stats[cls_name]))\n",
    "    if i % 3 == 0:\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(x, y, x_legend):\n",
    "    \"\"\"Plot accuracy as a function of x.\"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    plt.title('Classification accuracy as a function of %s' % x_legend)\n",
    "    plt.xlabel('%s' % x_legend)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.plot(x, y)\n",
    "\n",
    "\n",
    "rcParams['legend.fontsize'] = 10\n",
    "cls_names = list(sorted(cls_stats.keys()))\n",
    "\n",
    "# Plot accuracy evolution\n",
    "plt.figure()\n",
    "for _, stats in sorted(cls_stats.items()):\n",
    "    # Plot accuracy evolution with #examples\n",
    "    accuracy, n_examples = zip(*stats['accuracy_history'])\n",
    "    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim((0.8, 1))\n",
    "plt.legend(cls_names, loc='best')\n",
    "\n",
    "plt.figure()\n",
    "for _, stats in sorted(cls_stats.items()):\n",
    "    # Plot accuracy evolution with runtime\n",
    "    accuracy, runtime = zip(*stats['runtime_history'])\n",
    "    plot_accuracy(runtime, accuracy, 'runtime (s)')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim((0.8, 1))\n",
    "plt.legend(cls_names, loc='best')\n",
    "\n",
    "# Plot fitting times\n",
    "plt.figure()\n",
    "fig = plt.gcf()\n",
    "cls_runtime = []\n",
    "for cls_name, stats in sorted(cls_stats.items()):\n",
    "    cls_runtime.append(stats['total_fit_time'])\n",
    "\n",
    "cls_runtime.append(total_vect_time)\n",
    "cls_names.append('Vectorization')\n",
    "bar_colors = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n",
    "                     color=bar_colors)\n",
    "\n",
    "ax.set_xticks(np.linspace(0, len(cls_names) - 1, len(cls_names)))\n",
    "ax.set_xticklabels(cls_names, fontsize=10)\n",
    "ymax = max(cls_runtime) * 1.2\n",
    "ax.set_ylim((0, ymax))\n",
    "ax.set_ylabel('runtime (s)')\n",
    "ax.set_title('Training Times')\n",
    "\n",
    "\n",
    "def autolabel(rectangles):\n",
    "    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n",
    "    for rect in rectangles:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width() / 2.,\n",
    "                1.05 * height, '%.4f' % height,\n",
    "                ha='center', va='bottom')\n",
    "        plt.setp(plt.xticks()[1], rotation=30)\n",
    "\n",
    "\n",
    "autolabel(rectangles)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot prediction times\n",
    "plt.figure()\n",
    "cls_runtime = []\n",
    "cls_names = list(sorted(cls_stats.keys()))\n",
    "for cls_name, stats in sorted(cls_stats.items()):\n",
    "    cls_runtime.append(stats['prediction_time'])\n",
    "cls_runtime.append(parsing_time)\n",
    "cls_names.append('Read/Parse\\n+Feat.Extr.')\n",
    "cls_runtime.append(vectorizing_time)\n",
    "cls_names.append('Hashing\\n+Vect.')\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n",
    "                     color=bar_colors)\n",
    "\n",
    "ax.set_xticks(np.linspace(0, len(cls_names) - 1, len(cls_names)))\n",
    "ax.set_xticklabels(cls_names, fontsize=8)\n",
    "plt.setp(plt.xticks()[1], rotation=30)\n",
    "ymax = max(cls_runtime) * 1.2\n",
    "ax.set_ylim((0, ymax))\n",
    "ax.set_ylabel('runtime (s)')\n",
    "ax.set_title('Prediction Times (%d instances)' % n_test_documents)\n",
    "autolabel(rectangles)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
