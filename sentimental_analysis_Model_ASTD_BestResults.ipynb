{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Title :  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Folder:\n",
    "Data/tweet.txt\n",
    "Data/tweet_x.csv\n",
    "\n",
    "Libraries:\n",
    "sklearn\n",
    "scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from Definations import *\n",
    "from Utilities import *\n",
    "from sklearn.feature_selection.univariate_selection import SelectPercentile\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import isomap\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Benchmark Experiment Replication using ASTD. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It stands for Arabic Sentemental Tweet Dataset which is used for arabic social sentimental analysis .\n",
    "2. Consist of 10000 tweets gathered from twitter.\n",
    "3. Each tweet is Classified as: objective, subjective positive, subjective negative or subjective mixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Loading the data set in jupyter for exploration. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import supplementary visualization code visuals.py\n",
    "import visuals as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\rababalkhalifa\\appdata\\local\\conda\\conda\\envs\\py27\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\rababalkhalifa\\appdata\\local\\conda\\conda\\envs\\py27\\lib\\site-packages (from wordcloud) (1.15.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\rababalkhalifa\\appdata\\local\\conda\\conda\\envs\\py27\\lib\\site-packages (from wordcloud) (5.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import csv\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بعد استقالة رئيس #المحكمة_الدستورية ننتظر استقالة #رئيس_القضاء #السودان\tOBJ\r\n",
      "\n",
      "أهنئ الدكتور أحمد جمال الدين، القيادي بحزب مصر، بمناسبة صدور أولى روايته\tPOS\r\n",
      "\n",
      "البرادعي يستقوى بامريكا مرةاخرى و يرسل عصام العريان الي واشنطن شئ مقرف\tNEG\r\n",
      "\n",
      "#الحرية_والعدالة | شاهد الآن: #ليلة_الاتحادية أول فيلم استقصائي يتناول أسرار و كواليس تعرض لأول مرة حول حقيقة\tOBJ\r\n",
      "\n",
      "الوالدة لو اقولها بخاطري حشيشة تضحك بس من اقولها ملل الله وكيلك تعطيني محاضرة عن الفسق والفجور بجنوب الشيشان #ليه كذا يانبع الحنان\tNEUTRAL\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "filename = codecs.open('data\\Tweets.txt', 'r', encoding=\"utf-8\")\n",
    "outputfile = filename.readlines()\n",
    "for line in outputfile[0:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بعد استقالة رئيس #المحكمة_الدستورية ننتظر استق...</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>أهنئ الدكتور أحمد جمال الدين، القيادي بحزب مصر...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>البرادعي يستقوى بامريكا مرةاخرى و يرسل عصام ال...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#الحرية_والعدالة | شاهد الآن: #ليلة_الاتحادية ...</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الوالدة لو اقولها بخاطري حشيشة تضحك بس من اقول...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet Classification\n",
       "0  بعد استقالة رئيس #المحكمة_الدستورية ننتظر استق...            OBJ\n",
       "1  أهنئ الدكتور أحمد جمال الدين، القيادي بحزب مصر...            POS\n",
       "2  البرادعي يستقوى بامريكا مرةاخرى و يرسل عصام ال...            NEG\n",
       "3  #الحرية_والعدالة | شاهد الآن: #ليلة_الاتحادية ...            OBJ\n",
       "4  الوالدة لو اقولها بخاطري حشيشة تضحك بس من اقول...        NEUTRAL"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data\\Tweets_x.csv', delimiter='\\t',names= [\"Tweet\", \"Classification\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10006</td>\n",
       "      <td>9986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10002</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>يوسف الحسيني بالفيديو يفضح كذب قناة الجزيرة وي...</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>6675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tweet Classification\n",
       "count                                               10006           9986\n",
       "unique                                              10002              4\n",
       "top     يوسف الحسيني بالفيديو يفضح كذب قناة الجزيرة وي...            OBJ\n",
       "freq                                                    2           6675"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS it can be clearly seen, I lost some data in this process since i converted the txt into csv using excel. I removed them to keep dataset consistant but I will fix this issue later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Count of All Categories ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9986</td>\n",
       "      <td>9986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9982</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>الإخوان يطلقون طفايات الحريق للإيحاء بإلقاء ال...</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>6675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tweet Classification\n",
       "count                                                9986           9986\n",
       "unique                                               9982              4\n",
       "top     الإخوان يطلقون طفايات الحريق للإيحاء بإلقاء ال...            OBJ\n",
       "freq                                                    2           6675"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9986, 37515)\n",
      "6281\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    df['Tweet'], df['Classification'], test_size=0.5)\n",
    "\n",
    "\n",
    "# TASK: Build a vectorizer that splits strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(df['Tweet'])\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "#return how many word has occured in a document\n",
    "print(count_vect.vocabulary_.get(u'الحرية'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>دراما</th>\n",
       "      <td>17941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>استنتاجات</th>\n",
       "      <td>3519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الغربة</th>\n",
       "      <td>8174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>إفساد</th>\n",
       "      <td>2405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>شيفنا</th>\n",
       "      <td>20447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           frequency\n",
       "دراما          17941\n",
       "استنتاجات       3519\n",
       "الغربة          8174\n",
       "إفساد           2405\n",
       "شيفنا          20447"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(count_vect.vocabulary_)\n",
    "keys = np.array(list(count_vect.vocabulary_.keys()))\n",
    "values = np.array(list(count_vect.vocabulary_.values()))\n",
    "\n",
    "words_tokenized = pd.DataFrame(values,columns=['frequency'],index=keys) \n",
    "words_tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploraing tweet contints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('arabic')) \n",
    "  \n",
    "word_tokens = count_vect.vocabulary_.keys()\n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "print(len(word_tokens) )\n",
    "print(len(filtered_sentence ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Word Frequncy Visualization using wordcloud **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for w in word_tokens:\n",
    "    reshaped_text = arabic_reshaper.reshape(w)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    entries.append([bidi_text ,count_vect.vocabulary_.get(w) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating wordcloud. Relative scaling value is to adjust the importance of a frequency word.\n",
    "#See documentation: https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py\n",
    "wordcloud = WordCloud(font_path=\"font_aljazeera.ttf\").generate_from_frequencies(dict(entries))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for r in stop_words:\n",
    "    print (r)\n",
    "    if count > 10:\n",
    "        break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = open(\"data/tweets.txt\") \n",
    "outputfile = filename.readlines()\n",
    "for line in outputfile:\n",
    "    words = line.split() \n",
    "    for r in words: \n",
    "        if r in stop_words: \n",
    "            print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** we did not use the code below because we did not see any word in the stop word list **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = open(\"data/tweets.txt\") \n",
    "\n",
    "outputfile = filename.readlines()\n",
    "\n",
    "for line in outputfile:\n",
    "    \n",
    "    words = line.split() \n",
    "    \n",
    "    for r in words: \n",
    "        \n",
    "        if not r in stop_words: \n",
    "            \n",
    "            appendFile = open('data/ncleaned_tweets.txt','a') \n",
    "            \n",
    "            appendFile.write(\"\\t\"+r) \n",
    "            \n",
    "    appendFile.write(\"\\n\")\n",
    "    \n",
    "    appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s2 = df.Classification\n",
    "print(s2.value_counts())\n",
    "# s2.value_counts().plot(kind='hist') \n",
    "s2.value_counts().plot( kind=\"bar\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# y = np.random.rand(10,4)\n",
    "# y[:,0]= np.arange(10)\n",
    "# df = pd.DataFrame(y, columns=[\"X\", \"A\", \"B\", \"C\"])\n",
    "\n",
    "# ax = df.plot(x=\"X\", y=\"A\", kind=\"bar\")\n",
    "# df.plot(x=\"X\", y=\"B\", kind=\"bar\", ax=ax, color=\"C2\")\n",
    "# df.plot(x=\"X\", y=\"C\", kind=\"bar\", ax=ax, color=\"C3\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data PreProcessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ara = AraTweet()\n",
    "\n",
    "(Data,rating)=ara.read_clean_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "target_rating = le.fit_transform(rating)\n",
    "# list(le.inverse_transform([0,1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_rating.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rating.data[10005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String Token Count(Bag of Words):\n",
    "\n",
    "It takes each sentence (all the words) present in the data set in the review section and then splits each of the words present in the form of tokens. The occurrence of these tokens in the whole data set are counted in such a way that the count of the occurrence of each token in a positive and negative feedback (in balanced dataset) or positive, negative and neutral (in unbalanced dataset) are collected separately. Finally, the word frequency of the tokens is calculated.\n",
    "\n",
    "code:  https://stackoverflow.com/questions/653887/equivalent-for-linkedhashmap-in-python\n",
    "\n",
    "Ref: https://acadpubl.eu/hub/2018-119-12/articles/5/1211.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(Data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above ‘count_vect.fit_transform(twenty_train.data)’, create words/tokens dictionary. such that [n_samples, n_features]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency–inverse document frequency(Tfidf):\n",
    "\n",
    "\n",
    "It measures how important a word is to differentiate each category. It reduce the weightage of more common words like (stop words or common words) which occurs in all tweets. \n",
    "\n",
    "Code: https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-5-50b4e87d9bdd \n",
    "\n",
    "Ref: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Balanced Data using resample from Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASDA_data = []\n",
    "\n",
    "for t, r in zip(Data,rating):\n",
    "    ASDA_data.append([t,r])\n",
    "\n",
    "ASDA_Crp = pd.DataFrame(ASDA_data, columns=['Tweets', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(ASDA_Crp.rating.value_counts())\n",
    "# s2.value_counts().plot(kind='hist') \n",
    "ASDA_Crp.rating.value_counts().plot( kind=\"bar\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1- Downsampling - Balanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SRC: https://elitedatascience.com/imbalanced-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority_OBJ = ASDA_Crp[ASDA_Crp.rating=='OBJ']\n",
    "df_majority_NEG = ASDA_Crp[ASDA_Crp.rating=='NEG']\n",
    "df_majority_NEUTRAL = ASDA_Crp[ASDA_Crp.rating=='NEUTRAL']\n",
    "\n",
    "df_minority_POS = ASDA_Crp[ASDA_Crp.rating=='POS']\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_OBJ_downsampled = resample(df_majority_OBJ, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=799,     # to match minority class OBJ\n",
    "                                 random_state=0) # reproducible results\n",
    "\n",
    "df_majority_NEG_downsampled = resample(df_majority_NEG, \n",
    "                                 replace=False,    # sample without replacement \n",
    "                                 n_samples=799,     # to match minority class NEG\n",
    "                                 random_state=0) # reproducible results\n",
    "\n",
    "df_majority_NEUTRAL_downsampled = resample(df_majority_NEUTRAL, \n",
    "                                 replace=True,    # sample without replacement\n",
    "                                 n_samples=799,     # to match minority class NATURAL\n",
    "                                 random_state=0) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_OBJ_downsampled,df_majority_NEG_downsampled,df_majority_NEUTRAL_downsampled, df_minority_POS])\n",
    " \n",
    "# Display new class counts\n",
    "df_downsampled.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_downsampled.rating.value_counts())\n",
    "# s2.value_counts().plot(kind='hist') \n",
    "df_downsampled.rating.value_counts().plot( kind=\"bar\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2- UpSampling - Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority_OBJ = ASDA_Crp[ASDA_Crp.rating=='OBJ']\n",
    "\n",
    "df_minority_NEG = ASDA_Crp[ASDA_Crp.rating=='NEG']\n",
    "df_minority_NEUTRAL = ASDA_Crp[ASDA_Crp.rating=='NEUTRAL']\n",
    "df_minority_POS = ASDA_Crp[ASDA_Crp.rating=='POS']\n",
    " \n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_NEG_upsampled = resample(df_minority_NEG, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=6691,    # to match majority class\n",
    "                                 random_state=0) # reproducible results\n",
    "# Upsample minority class\n",
    "df_minority_NEUTRAL_upsampled = resample(df_minority_NEUTRAL, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=6691,    # to match majority class\n",
    "                                 random_state=0) # reproducible results\n",
    "# Upsample minority class\n",
    "df_minority_POS_upsampled = resample(df_minority_POS, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=6691,    # to match majority class\n",
    "                                 random_state=0) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority_OBJ, df_minority_POS_upsampled,df_minority_NEUTRAL_upsampled,df_minority_NEG_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.rating.value_counts()\n",
    "# 1    576\n",
    "# 0    576\n",
    "# Name: balance, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_upsampled.rating.value_counts())\n",
    "# s2.value_counts().plot(kind='hist') \n",
    "df_upsampled.rating.value_counts().plot( kind=\"bar\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataDistCollection = {'up_balanced': df_upsampled ,'down_balanced': df_downsampled , 'unBalanced': ASDA_Crp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Machine implementation using scikit-learn ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "models = [\n",
    "    LogisticRegression(random_state=0),\n",
    "    PassiveAggressiveClassifier(random_state=0),\n",
    "    LinearSVC(tol=1e-3, random_state=0),\n",
    "    Perceptron(tol=.2,random_state=0),\n",
    "    BernoulliNB(binarize=0.5),\n",
    "    SGDClassifier(loss=\"hinge\", penalty=\"l2\", random_state=0),\n",
    "    KNeighborsClassifier(n_neighbors=5, metric='euclidean'),\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    DecisionTreeClassifier(),\n",
    "    MultinomialNB()\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, X_train_tfidf, rating, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "        \n",
    "        \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df[cv_df['accuracy'] > .68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=10, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fine tune the all the models ###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function to retrive sentimental count in each class\n",
    "def GetCount(array):\n",
    "    df = pd.DataFrame(array,columns= [\"Class\"])\n",
    "    counts = df.Class.value_counts()\n",
    "    OBJ = counts['OBJ']\n",
    "    NEG = counts['NEG']\n",
    "    NEUTRAL = counts['NEUTRAL']\n",
    "    POS = counts['POS']\n",
    "    return [OBJ, NEG, NEUTRAL ,POS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "gr = AraTweet()\n",
    "\n",
    "\n",
    "classifiers_collection = {}\n",
    "scores_collection = {}\n",
    "dataframe_collection = {}\n",
    "TweetCount_collection = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "FeatuerDataSet = []\n",
    "for key in DataDistCollection.keys():\n",
    "    scores = list()\n",
    "    # Split the 'features' and 'income' data into training and testing sets\n",
    "    d_train, d_test, y_train, y_test = train_test_split(DataDistCollection[key].Tweets.values, \n",
    "                                                    DataDistCollection[key].rating.values, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "    # Show the results of the split\n",
    "    # print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "    # print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------\n",
    "    TweetCount = [GetCount(y_train ), GetCount(y_test) ]\n",
    "    df_tweetCount = pd.DataFrame(TweetCount,columns= ['OBJ', 'NEG', 'NEUTRAL' ,'POS']) \n",
    "    df_tweetCount['Dataset'] = ['Train Set','Test Set']\n",
    "    df_tweetCount['name'] = key\n",
    "    TweetCount_collection[key] = df_tweetCount\n",
    "    #---------------------------------------------------------------------------------\n",
    "    for feat_generator in Features_Generators:\n",
    "            ####################################Features Generation#############################\n",
    "    #         print(\"Features Generation:\", feat_generator['name'])\n",
    "            X_train = feat_generator['feat_generator'].fit_transform(d_train)\n",
    "            X_test = feat_generator['feat_generator'].transform(d_test)\n",
    "\n",
    "\n",
    "            for clf in classifiers:\n",
    "                    print(\"\\n------\\n\")\n",
    "                    print(\"tuning: \", clf[\"name\"] )\n",
    "\n",
    "\n",
    "                    grid_fit = clf['tune_clf'].fit(X_train, y_train)\n",
    "                    best_clf = grid_fit.best_estimator_\n",
    "                    # Make predictions using the unoptimized and model\n",
    "                    predictions = (clf[\"clf\"].fit(X_train, y_train)).predict(X_test)\n",
    "                    best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "\n",
    "                    # Report the before-and-afterscores\n",
    "                    Clf_name = clf[\"name\"] \n",
    "\n",
    "                    UnOpt_accuracy = accuracy_score(y_test, predictions)\n",
    "                    UnOpt_fScore = f1_score(y_test, predictions,average='macro')\n",
    "                    Opt_accuracy = accuracy_score(y_test, best_predictions)\n",
    "                    Opt_fScore = f1_score(y_test, best_predictions,average='macro')\n",
    "\n",
    "\n",
    "            #         clasifiers.append(Clf_name)\n",
    "\n",
    "                    if (UnOpt_accuracy < Opt_accuracy ):\n",
    "                        Best_Settings = grid_fit.best_params_\n",
    "                        datarow = [Clf_name,key,feat_generator['name'],Best_Settings,UnOpt_accuracy , UnOpt_fScore , Opt_accuracy , Opt_fScore]\n",
    "                        print(Clf_name,'  ',key , '  ',feat_generator['name'],'  ',Opt_accuracy,'  ' , Opt_fScore )\n",
    "#                         (acc, tacc, support, f1 , df) = Evaluate_Result(best_predictions, y_test)\n",
    "                    else:\n",
    "                        Best_Settings = clf[\"clf\"].get_params()\n",
    "                        datarow = [Clf_name,key,feat_generator['name'],Best_Settings,Opt_accuracy , Opt_fScore , UnOpt_accuracy , UnOpt_fScore]\n",
    "#                         (acc, tacc, support, f1 , df) = Evaluate_Result(predictions, y_test)\n",
    "                        \n",
    "                        print(Clf_name,'  ',key , '  ',feat_generator['name'],'  ',UnOpt_accuracy,'  ' , UnOpt_fScore )\n",
    "                        \n",
    "                    \n",
    "                    results.append(datarow)\n",
    "\n",
    "\n",
    "                    score = dict(data=key,\n",
    "                                             feat_generator=feat_generator['name'],\n",
    "                                             clf=clf['name'],\n",
    "                                             # feat_ext=feat_ext['name'],\n",
    "                                             f1=f1,\n",
    "                                             acc=acc,\n",
    "                                             tacc=tacc)\n",
    "\n",
    "                    df['Classifier'] =  clf[\"name\"]\n",
    "                    df['feauter_generator'] = feat_generator['name']\n",
    "\n",
    "                    df['acc'] = acc\n",
    "                    df['f1']=f1\n",
    "\n",
    "                    dataframe_collection[ key + '-' + clf[\"name\"] + '-' + feat_generator['name'] ]  = df\n",
    "    #                         print(clf[\"name\"],df['feauter_generator'] )\n",
    "                    scores.append(score)\n",
    "        \n",
    "    for key in dataframe_collection.keys():\n",
    "        print(\"\\n\" +\"=\"*40)\n",
    "        print(key)\n",
    "        print(\"-\"*40)\n",
    "        print(dataframe_collection[key])\n",
    "\n",
    "        df_Classifiers = pd.DataFrame(classifiers)\n",
    "        classifiers_collection[key] = df_Classifiers\n",
    "\n",
    "    #     print(\"-\"*40)\n",
    "    #     print(df_Classifiers.head())\n",
    "\n",
    "        df_s = pd.DataFrame(scores)\n",
    "        scores_collection[key] = df_s\n",
    "        \n",
    "df_feauterCount = pd.DataFrame(FeatuerDataSet, columns=['DataSet_Name','Feauter_Generator','Feauters_Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Performance Measures for Classifieres ###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_resutlts = pd.DataFrame(results,columns=['clasifiers','Data','Feauter_Generator' ,'Best_Settings' , 'UnOpt_accuracys' , 'UnOpt_fScores' , 'Opt_accuracys' , 'Opt_fScores'])\n",
    "# df_resutlts.index = df_resutlts.clasifiers.get_values()\n",
    "df_resutlts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_resutlts[df_resutlts['Opt_accuracys'] > .97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df_resutlts[df_resutlts['Opt_accuracys'] > .70]\n",
    "\n",
    "d = subset[subset['Opt_fScores'] < .50]\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = df_resutlts[['Opt_accuracys','UnOpt_accuracys']]\n",
    "ax = dataset.plot(figsize=(20,10))\n",
    "ax.set_xticklabels( df_resutlts.clasifiers.get_values())\n",
    "# ax.set_xticks([1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# ax.set_xlim([0, 10])\n",
    "# ax.set_xlabel(\"Classifiers\" )\n",
    "# ax.set_ylabel(\"Accuracy\")\n",
    "# ax.set_title(\"Visualizing Results using Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {
   "attach-environment": true,
   "summary": "This is my model "
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
